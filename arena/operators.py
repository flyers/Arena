import mxnet as mx
import mxnet.ndarray as nd
import numpy
from scipy.stats import entropy

from utils import *


# TODO Backward using NDArray will cause some troubles see `https://github.com/dmlc/mxnet/issues/1720'
class DQNOutput(mx.operator.CustomOp):
    def __init__(self):
        super(DQNOutput, self).__init__()

    def forward(self, is_train, req, in_data, out_data, aux):
        self.assign(out_data[0], req[0], in_data[0])

    def backward(self, req, out_grad, in_data, out_data, in_grad, aux):
        x = out_data[0].asnumpy()
        action = in_data[1].asnumpy().astype(numpy.int)
        reward = in_data[2].asnumpy()
        dx = in_grad[0]
        ret = numpy.zeros(shape=dx.shape, dtype=numpy.float32)
        ret[numpy.arange(action.shape[0]), action] \
            = numpy.clip(x[numpy.arange(action.shape[0]), action] - reward, -1, 1)
        self.assign(dx, req[0], ret)


@mx.operator.register("DQNOutput")
class DQNOutputProp(mx.operator.CustomOpProp):
    def __init__(self):
        super(DQNOutputProp, self).__init__(need_top_grad=False)

    def list_arguments(self):
        return ['data', 'action', 'reward']

    def list_outputs(self):
        return ['output']

    def infer_shape(self, in_shape):
        data_shape = in_shape[0]
        action_shape = (in_shape[0][0],)
        reward_shape = (in_shape[0][0],)
        output_shape = in_shape[0]
        return [data_shape, action_shape, reward_shape], [output_shape], []

    def create_operator(self, ctx, shapes, dtypes):
        return DQNOutput()


class DQNOutputNpyOp(mx.operator.NumpyOp):
    def __init__(self):
        super(DQNOutputNpyOp, self).__init__(need_top_grad=False)

    def list_arguments(self):
        return ['data', 'action', 'reward']

    def list_outputs(self):
        return ['output']

    def infer_shape(self, in_shape):
        data_shape = in_shape[0]
        action_shape = (in_shape[0][0],)
        reward_shape = (in_shape[0][0],)
        output_shape = in_shape[0]
        return [data_shape, action_shape, reward_shape], [output_shape]

    def forward(self, in_data, out_data):
        x = in_data[0]
        y = out_data[0]
        y[:] = x

    def backward(self, out_grad, in_data, out_data, in_grad):
        x = out_data[0]
        action = in_data[1].astype(numpy.int)
        reward = in_data[2]
        dx = in_grad[0]
        dx[:] = 0
        dx[numpy.arange(action.shape[0]), action] \
            = numpy.clip(x[numpy.arange(action.shape[0]), action] - reward, -1, 1)


'''
Name: dqn_sym_nips
Usage: Structure of the Deep Q Network in the NIPS 2013 workshop paper:
      "Playing Atari with Deep Reinforcement Learning" (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)
'''


def dqn_sym_nips(action_num, data=None, name='dqn'):
    if data is None:
        net = mx.symbol.Variable('data')
    else:
        net = data
    net = mx.symbol.Convolution(data=net, name='conv1', kernel=(8, 8), stride=(4, 4), num_filter=16)
    net = mx.symbol.Activation(data=net, name='relu1', act_type="relu")
    net = mx.symbol.Convolution(data=net, name='conv2', kernel=(4, 4), stride=(2, 2), num_filter=32)
    net = mx.symbol.Activation(data=net, name='relu2', act_type="relu")
    net = mx.symbol.Flatten(data=net)
    net = mx.symbol.FullyConnected(data=net, name='fc3', num_hidden=256)
    net = mx.symbol.Activation(data=net, name='relu3', act_type="relu")
    net = mx.symbol.FullyConnected(data=net, name='fc4', num_hidden=action_num)
    net = mx.symbol.Custom(data=net, name=name, op_type='DQNOutput')
    return net


'''
Name: dqn_sym_nature
Usage: Structure of the Deep Q Network in the Nature 2015 paper:
Human-level control through deep reinforcement learning
(http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html)
'''


def dqn_sym_nature(action_num, data=None, name='dqn'):
    if data is None:
        net = mx.symbol.Variable('data')
    else:
        net = data
    net = mx.symbol.Variable('data')
    net = mx.symbol.Convolution(data=net, name='conv1', kernel=(8, 8), stride=(4, 4), num_filter=32)
    net = mx.symbol.Activation(data=net, name='relu1', act_type="relu")
    net = mx.symbol.Convolution(data=net, name='conv2', kernel=(4, 4), stride=(2, 2), num_filter=64)
    net = mx.symbol.Activation(data=net, name='relu2', act_type="relu")
    net = mx.symbol.Convolution(data=net, name='conv3', kernel=(3, 3), stride=(1, 1), num_filter=64)
    net = mx.symbol.Activation(data=net, name='relu3', act_type="relu")
    net = mx.symbol.Flatten(data=net)
    net = mx.symbol.FullyConnected(data=net, name='fc4', num_hidden=512)
    net = mx.symbol.Activation(data=net, name='relu4', act_type="relu")
    net = mx.symbol.FullyConnected(data=net, name='fc5', num_hidden=action_num)
    net = mx.symbol.Custom(data=net, name=name, op_type='DQNOutput')
    return net


'''
Name: LogNormalPolicy
Usage: This OP outputs actions generated by a policy with normal distribution.
       The loss function for backward operation is set to be \sum_i - log(N(a_i|m_i, v_i)) * R_i
'''


class LogNormalPolicy(mx.operator.CustomOp):
    def __init__(self, deterministic, implicit_backward, grad_scale):
        super(LogNormalPolicy, self).__init__()
        self.deterministic = deterministic
        self.implicit_backward = implicit_backward
        self.grad_scale = grad_scale

    def forward(self, is_train, req, in_data, out_data, aux):
        # TODO There seems to be some problems when I try to use `mx.random.normal`
        # mean = in_data[0]
        # var = in_data[1]
        # if self.deterministic == True:
        #     self.assign(out_data[0], req[0], mean)
        # else:
        #     self.assign(out_data[0], req[0], mean + nd.sqrt(var) * mx.random.normal(0, 1, shape=mean.shape,
        #                                                                             ctx=mean.context))
        mean = in_data[0].asnumpy()
        var = in_data[1].asnumpy()
        if self.deterministic:
            self.assign(out_data[0], req[0], nd.array(mean))
        else:
            self.assign(out_data[0], req[0], nd.array(numpy.sqrt(var) * get_numpy_rng().randn(*mean.shape) + mean))

    def backward(self, req, out_grad, in_data, out_data, in_grad, aux):
        mean = in_data[0]
        var = in_data[1]
        if self.implicit_backward:
            action = out_data[0]
        else:
            action = in_data[3]
        score = in_data[2]
        grad_mu = in_grad[0]
        grad_var = in_grad[1]
        self.assign(grad_mu, req[0], - (action - mean) * score.reshape((score.shape[0], 1)) * self.grad_scale / var)
        self.assign(grad_var, req[1], - nd.square(action - mean) * score.reshape((score.shape[0], 1)) *
                    self.grad_scale / (2 * nd.square(var)))
        # mean = in_data[0].asnumpy()
        # var = in_data[1].asnumpy()
        # action = out_data[0].asnumpy()
        # score = in_data[2].asnumpy()
        # grad_mu = in_grad[0]
        # grad_var = in_grad[1]
        # self.assign(grad_mu, req[0], nd.array(- (action - mean) * score.reshape((score.shape[0], 1)) / var))
        # self.assign(grad_var, req[1], nd.array(- numpy.square(action - mean) * score.reshape((score.shape[0], 1)) \
        #                   / numpy.square(var) / 2))


@mx.operator.register("LogNormalPolicy")
class LogNormalPolicyProp(mx.operator.CustomOpProp):
    def __init__(self, deterministic=0, implicit_backward=1, grad_scale=1.0):
        super(LogNormalPolicyProp, self).__init__(need_top_grad=False)
        self.deterministic = safe_eval(deterministic)
        self.implicit_backward = safe_eval(implicit_backward)
        self.grad_scale = safe_eval(grad_scale)

    def list_arguments(self):
        if self.implicit_backward:
            return ['mean', 'var', 'score']
        else:
            return ['mean', 'var', 'score', 'backward_action']

    def list_outputs(self):
        return ['output']

    def infer_shape(self, in_shape):
        mean_shape = in_shape[0]
        var_shape = in_shape[1]
        score_shape = (in_shape[0][0],)
        output_shape = in_shape[0]
        if self.implicit_backward:
            return [mean_shape, var_shape, score_shape], [output_shape], []
        else:
            backward_action_shape = in_shape[0]
            return [mean_shape, var_shape, score_shape, backward_action_shape], [output_shape], []

    def create_operator(self, ctx, shapes, dtypes):
        return LogNormalPolicy(deterministic=self.deterministic,
                               implicit_backward=self.implicit_backward,
                               grad_scale=self.grad_scale)


'''
Name: LogLaplacePolicy
Usage: This OP outputs actions generated by a policy with laplace distribution.
       The loss function for backward operation is set to be \sum_i - log(Laplace(a_i|\mu_i, b_i)) * R_i
       Laplace Distribution: \frac{1}{2b} exp(-\frac{\abs{x - \mu}{b})
'''


class LogLaplacePolicy(mx.operator.NumpyOp):
    def __init__(self, rng=get_numpy_rng(), deterministic=False):
        super(LogLaplacePolicy, self).__init__(need_top_grad=False)
        self.rng = rng
        self.deterministic = deterministic

    def list_arguments(self):
        return ['mean', 'scale', 'score']

    def list_outputs(self):
        return ['output']

    def infer_shape(self, in_shape):
        mean_shape = in_shape[0]
        scale_shape = in_shape[1]
        score_shape = (in_shape[0][0],)
        output_shape = in_shape[0]
        return [mean_shape, scale_shape, score_shape], \
               [output_shape]

    def forward(self, in_data, out_data):
        mean = in_data[0]
        scale = in_data[1]
        if self.deterministic:
            out_data[0][:] = mean
        else:
            out_data[0][:] = self.rng.laplace(loc=mean, scale=scale)

    def backward(self, out_grad, in_data, out_data, in_grad):
        mean = in_data[0]
        scale = in_data[1]
        action = out_data[0]
        score = in_data[2]
        grad_mu = in_grad[0]
        grad_scale = in_grad[1]
        grad_mu[:] = numpy.sign(mean - action) / scale * score.reshape((score.shape[0], 1))
        grad_scale[:] = - numpy.abs(mean - action) / (scale * scale) * score.reshape((score.shape[0], 1))


'''
Name: LogSoftmaxPolicy
Usage: This OP outputs actions generated by a multinomial distribution whose parameters are computed
       by applying softmax to the input.
       The loss function for backward operation is set to be
          \sum_i - log(multinomial(a_i| softmax(x_i)) * R_i - H(multinomial(a_i|softmax(x_i)))
'''

class LogSoftmaxPolicy(mx.operator.CustomOp):
    def __init__(self, deterministic, implicit_backward, use_mask, entropy_regularization, grad_scale):
        super(LogSoftmaxPolicy, self).__init__()
        self.deterministic = deterministic
        self.implicit_backward = implicit_backward
        self.use_mask = use_mask
        self.entropy_regularization = entropy_regularization
        self.grad_scale = grad_scale

    def forward(self, is_train, req, in_data, out_data, aux):
        x = in_data[0].asnumpy()
        y_prob = out_data[1].asnumpy()
        if self.use_mask:
            mask = in_data[1].asnumpy()
            mask = (mask > 0).astype(numpy.float32)
            y_prob[:] = mask * numpy.exp(x - numpy.ma.array(x, mask=1 - mask).max(axis=1).data.reshape((x.shape[0], 1)))
        else:
            y_prob[:] = numpy.exp(x - x.max(axis=1).reshape((x.shape[0], 1)))
        y_prob /= y_prob.sum(axis=1).reshape((x.shape[0], 1))
        if self.deterministic:
            self.assign(out_data[0], req[0], nd.array(numpy.argmax(y_prob, axis=1)))
            self.assign(out_data[1], req[1], nd.array(y_prob))
        else:
            y_action = out_data[0].asnumpy()
            for ind in range(y_action.shape[0]):
                y_action[ind] = numpy.searchsorted(numpy.cumsum(y_prob[ind]), get_numpy_rng().rand())
            self.assign(out_data[0], req[0], nd.array(y_action))
            self.assign(out_data[1], req[1], nd.array(y_prob))

    def backward(self, req, out_grad, in_data, out_data, in_grad, aux):
        if self.use_mask:
            score = in_data[2].asnumpy()
        else:
            score = in_data[1].asnumpy()
        if self.implicit_backward:
            action = out_data[0].asnumpy().astype(numpy.int32)
        else:
            action = in_data[2].asnumpy().astype(numpy.int32)
        prob = out_data[1].asnumpy()
        dx = in_grad[0].asnumpy()
        dx[:] = prob
        dx[numpy.arange(action.shape[0]), action] -= 1.0
        dx[:] *= score.reshape(score.shape[0], 1)
        dx[:] += self.entropy_regularization * (prob * (entropy(prob.T).reshape(prob.shape[0], 1)) +
                                                numpy.nan_to_num(prob * numpy.log(prob)))
        self.assign(in_grad[0], req[0], self.grad_scale * dx)


@mx.operator.register("LogSoftmaxPolicy")
class LogSoftmaxPolicyProp(mx.operator.CustomOpProp):
    def __init__(self, deterministic=False, implicit_backward=True, use_mask=False,
                 entropy_regularization=0.01, grad_scale=1.0):
        super(LogSoftmaxPolicyProp, self).__init__(need_top_grad=False)
        self.deterministic = safe_eval(deterministic)
        self.implicit_backward = safe_eval(implicit_backward)
        self.use_mask = safe_eval(use_mask)
        self.entropy_regularization = safe_eval(entropy_regularization)
        self.grad_scale = safe_eval(grad_scale)

    def list_arguments(self):
        if self.implicit_backward:
            if self.use_mask:
                return ['data', 'mask', 'score']
            else:
                return ['data', 'score']
        else:
            if self.use_mask:
                return ['data', 'mask', 'score', 'backward_action']
            else:
                return ['data', 'score', 'backward_action']

    def list_outputs(self):
        return ['action', 'prob']

    def infer_shape(self, in_shape):
        data_shape = in_shape[0]
        score_shape = (in_shape[0][0],)
        action_shape = (in_shape[0][0],)
        prob_shape = in_shape[0]
        if self.implicit_backward:
            if self.use_mask:
                mask_shape = in_shape[0]
                return [data_shape, mask_shape, score_shape], [action_shape, prob_shape], []
            else:
                return [data_shape, score_shape], [action_shape, prob_shape], []
        else:
            backward_action_shape = (in_shape[0][0],)
            if self.use_mask:
                mask_shape = in_shape[0]
                return [data_shape, mask_shape, score_shape, backward_action_shape], [action_shape, prob_shape], []
            else:
                return [data_shape, score_shape, backward_action_shape], [action_shape, prob_shape], []

    def create_operator(self, ctx, shapes, dtypes):
        return LogSoftmaxPolicy(deterministic=self.deterministic,
                                implicit_backward=self.implicit_backward,
                                use_mask=self.use_mask,
                                entropy_regularization=self.entropy_regularization,
                                grad_scale=self.grad_scale)


def spatial_softmax(data, channel_num, rows, cols, name=None):
    out = mx.symbol.Reshape(data, target_shape=(0, rows*cols))
    out = mx.symbol.SoftmaxActivation(data=out, mode='instance')
    if name is None:
        out = mx.symbol.Reshape(data=out, target_shape=(0, channel_num, rows, cols))
    else:
        out = mx.symbol.Reshape(data=out, target_shape=(0, channel_num, rows, cols), name=name)
    return out