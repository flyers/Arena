__author__ = 'flyers'

from arena.games import VREPGame
from arena.utils import *
from arena.operators import *
from arena.games.vrep import helper
from arena import Base
import os
import numpy
import time
import mxnet as mx
import mxnet.ndarray as nd
import matplotlib.pyplot as plt

'''
Name: LinearRegressionPolicy
Usage: This OP outputs policy generated by linear regression.
       The loss function for backward operation is set to l2 loss: 1/N * \sum_i (a_i - y_i)^2
'''
class LinearRegressionPolicy(mx.operator.NumpyOp):
    def __init__(self):
        super(LinearRegressionPolicy, self).__init__(need_top_grad=False)

    def list_arguments(self):
        return ['data', 'target']

    def list_outputs(self):
        return ['output']

    def infer_shape(self, in_shape):
        data_shape = in_shape[0]
        target_shape = (in_shape[0][0], )
        output_shape = in_shape[0]
        return [data_shape, target_shape], \
                   [output_shape]

    def forward(self, in_data, out_data):
        x = in_data[0]
        y = out_data[0]
        y[:] = x

    def backward(self, out_grad, in_data, out_data, in_grad):
        x = out_data[0]
        y = in_data[1]
        dx = in_grad[0]
        dx[:] = 0
        dx[:] = (x - y)


def regression_policy_sym(action_num):
    data = mx.symbol.Variable('data')
    net_mean = mx.symbol.FullyConnected(data=data, name='fc_mean_1', num_hidden=64)
    net_mean = mx.symbol.Activation(data=net_mean, name='fc_mean_relu_1', act_type='relu')
    net_mean = mx.symbol.FullyConnected(data=net_mean, name='fc_mean_2', num_hidden=32)
    net_mean = mx.symbol.Activation(data=net_mean, name='fc_mean_relu_2', act_type='relu')
    net_mean = mx.symbol.FullyConnected(data=net_mean, name='fc_mean_3', num_hidden=action_num)
    target = mx.symbol.Variable('regression_label')
    net_mean = mx.symbol.LinearRegressionOutput(data=net_mean, label=target)
    # net_mean = output_op(data=net_mean, name='policy')
    return net_mean


def load_data(path):
    _, body_v = helper.read_linear_velocity(path)
    _, body_w = helper.read_angular_velocity(path)
    return numpy.concatenate((body_v, body_w), axis=1), helper.read_motor(path)


action_dim = 4
state_dim = 6

ctx = mx.gpu()
epoch_num = 100
minibatch_size = 64

data_shapes = {'data': (minibatch_size, state_dim),
               'regression_label': (minibatch_size, action_dim)}


# regression_output_op = LogNormalPolicy()

policy_net_mean = regression_policy_sym(action_dim)
# policy_net_mean = regression_policy_sym(action_dim, regression_output_op)

net = Base(data_shapes=data_shapes, sym=policy_net_mean, name='PolicyNet',
           initializer=mx.initializer.Xavier(factor_type='in', magnitude=1.0), ctx=ctx)
optimizer = mx.optimizer.create(name='sgd', learning_rate=0.00001,
                                clip_gradient=None, rescale_grad=1.0, wd=0.)
updater = mx.optimizer.get_updater(optimizer)

net.print_stat()

# preparing training data
data_path = '/home/sliay/Documents/V-REP_PRO_EDU_V3_2_3_rev4_64_Linux/data/pretrain'
state_data, action_data = load_data(data_path)
# random shuffle the data
shuffle_index = get_numpy_rng().permutation(state_data.shape[0])
state_data = state_data[shuffle_index, :]
action_data = action_data[shuffle_index, :]
data_size = state_data.shape[0]
batch_num = numpy.ceil(float(data_size) / minibatch_size)

for epoch in xrange(epoch_num):
    batch_count = 0
    while batch_count < batch_num:
        indices = numpy.arange(batch_count * minibatch_size,
                               min(data_size, (batch_count+1) * minibatch_size))
        data_batch = state_data[indices, :]
        target_batch = action_data[indices, :]

        val_action = net.forward(is_train=True, batch_size=data_batch.shape[0],
                                 data=data_batch, regression_label=target_batch)[0].asnumpy
        net.backward(batch_size=data_batch.shape[0], regression_label=target_batch)
        # net.backward(batch_size=data_batch.shape[0], target=target_batch)
        net.update(updater)

    net.save_params('./models/', epoch=epoch)



