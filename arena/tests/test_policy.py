import mxnet as mx
import mxnet.ndarray as nd
import numpy
from arena import Base
from arena.utils import *
import matplotlib.pyplot as plt
from arena.operators import *


'''
Name: SoftmaxPolicy
Usage: This OP outputs actions generated by the parameteric form \pi(a|s) = softmax(.)
       The loss function for backward operation is set to be
           \sum action_grad * \partial_theta \pi(a|s)

softmax(a) * (action_grad - (softmax(a) * out_grad).sum(axis=1))
grad[:] = softmax(a) * action_grad
grad[:] -= softmax(a) * grad.sum(axis=1, keepdims=True)

'''
class SoftmaxPolicy(mx.operator.NumpyOp):
    def __init__(self):
        super(SoftmaxPolicy, self).__init__(need_top_grad=False)

#TODO
class ParticlePolicyOut(mx.operator.NumpyOp):
    def __init__(self):
        super(ParticlePolicyOut, self).__init__(need_top_grad=False)

def policy_sym(action_num, output_op):
    net = mx.symbol.Variable('data')
    net = mx.symbol.FullyConnected(data=net, name='fc', num_hidden=action_num)
    net = output_op(data=net, name='dqn')
    return net


def simple_game(data, action):
    return (numpy.square(action - data*data).sum(axis=1) < 2)*1000 + \
           (numpy.square(action - data*data).sum(axis=1) < 10)*100 + 1


def simple_game_discrete(data, action):
    return (action == numpy.argmax(data*data, axis=1)) * 100 + 1

def update_line(hl, fig, ax, new_x, new_y):
    hl.set_xdata(numpy.append(hl.get_xdata(), new_x))
    hl.set_ydata(numpy.append(hl.get_ydata(), new_y))
    ax.relim()
    ax.autoscale_view()
    fig.canvas.draw()
    fig.canvas.flush_events()

def test_lognormal():
    var = mx.symbol.Variable('var')
    data = mx.symbol.Variable('data')
    net_mean = mx.symbol.FullyConnected(data=data, name='fc_mean_1', num_hidden=20)
    net_mean = mx.symbol.Activation(data=net_mean, name='fc_mean_relu_1', act_type='relu')
    net_mean = mx.symbol.FullyConnected(data=data, name='fc_mean_2', num_hidden=20)
    net_mean = mx.symbol.Activation(data=net_mean, name='fc_mean_relu_2', act_type='relu')
    net_mean = mx.symbol.FullyConnected(data=net_mean, name='fc_mean_3', num_hidden=10)
    net_var = mx.symbol.FullyConnected(data=data, name='fc_var_1', num_hidden=10)
    net_var = mx.symbol.Activation(data=net_var, name='fc_var_softplus_1', act_type='softrelu')
    net = mx.symbol.Custom(mean=net_mean, var=net_var, name='policy', deterministic=False, op_type='LogNormalPolicy')
    ctx = mx.gpu()
    minibatch_size = 100
    data_shapes = {'data': (minibatch_size, 10), 'policy_score': (minibatch_size,)} #, 'var':(minibatch_size,)}
    qnet = Base(data_shapes=data_shapes, sym=net, name='PolicyNet',
                initializer=mx.initializer.Xavier(factor_type="in", magnitude=1.0),
                ctx=ctx)
    print qnet.internal_sym_names

    lr = 0.00001
    optimizer = mx.optimizer.create(name='sgd', learning_rate=0.00001,
                                    clip_gradient=None,
                                    rescale_grad=1.0, wd=0.)
    updater = mx.optimizer.get_updater(optimizer)
    total_iter = 1000000
    stats = numpy.zeros((total_iter, 3), dtype=numpy.float32)
    plt.ion()
    fig, ax = plt.subplots()
    lines, = ax.plot([], [])
    ax.set_autoscaley_on(True)
    baseline = 0
    for i in range(total_iter):
    #    for k, v in qnet.params.items():
    #        print k, v.asnumpy()
        data = numpy.random.randn(minibatch_size, 10)
        means = qnet.forward(batch_size=minibatch_size, sym_name="fc_mean_3_output", data=data)[0].asnumpy()
        vars = qnet.forward(batch_size=minibatch_size, sym_name="fc_var_softplus_1_output", data=data)[0].asnumpy()

        outputs = qnet.forward(batch_size=minibatch_size, is_train=True, data=data) #, var=0.5*numpy.ones((minibatch_size, )))
        action = outputs[0].asnumpy()
        score = simple_game(data, action)
        baseline = baseline - 0.001 * (baseline - score.mean())
        print 'score=', score.mean(), 'err=', numpy.square(means - data*data).mean(), 'var=', vars.mean(), 'baseline=', baseline
        stats[i] = [score.mean(), numpy.square(means - data*data).mean(), vars.mean()]
        qnet.backward(batch_size=minibatch_size, policy_score=score-baseline)
        qnet.update(updater)
        update_line(lines, fig, ax, i, score.mean())#numpy.square(means - data*data).mean())

def test_logsoftmax():
    var = mx.symbol.Variable('var')
    data = mx.symbol.Variable('data')
    net = mx.symbol.FullyConnected(data=data, name='fc1', num_hidden=10)
    net = mx.symbol.Activation(data=net, name='relu1', act_type='relu')
    net = mx.symbol.FullyConnected(data=net, name='fc2', num_hidden=4)
    net = mx.symbol.Custom(data=net, name='policy', op_type='LogSoftmaxPolicy')
    ctx = mx.gpu()
    minibatch_size = 100
    data_shapes = {'data': (minibatch_size, 4),
                   'policy_score': (minibatch_size,)}
    qnet = Base(data_shapes=data_shapes, sym=net, name='PolicyNet',
                initializer=mx.initializer.Xavier(factor_type="in", magnitude=1.0),
                ctx=ctx)
    print qnet.internal_sym_names

    lr = 0.00001
    optimizer = mx.optimizer.create(name='sgd', learning_rate=0.00001,
                                    clip_gradient=None,
                                    rescale_grad=1.0, wd=0.)
    updater = mx.optimizer.get_updater(optimizer)
    total_iter = 1000000
    stats = numpy.zeros((total_iter, 3), dtype=numpy.float32)
    plt.ion()
    fig, ax = plt.subplots()
    lines, = ax.plot([], [])
    ax.set_autoscaley_on(True)
    baseline = 0
    for i in range(total_iter):
        data = numpy.random.randn(minibatch_size, 4)
        outputs = qnet.forward(batch_size=minibatch_size, is_train=True, data=data)
        action = outputs[0].asnumpy()
        prob = outputs[1].asnumpy()
        #print 'data=', data, 'action=', action, 'prob=', prob
        #ch = raw_input()
        score = simple_game_discrete(data, action)
        baseline = baseline - 0.001 * (baseline - score.mean())
        print 'score=', score.mean(), 'acc=', numpy.sum(
            action == numpy.argmax(data * data, axis=1)).mean(), 'baseline=', baseline
        stats[i] = [score.mean(), numpy.sum(action == numpy.argmax(data * data, axis=1)).mean(), baseline]
        qnet.backward(batch_size=minibatch_size, policy_score=score - baseline)
        qnet.update(updater)
        update_line(lines, fig, ax, i, score.mean())  # numpy.square(means - data*data).mean())
test_lognormal()
#test_logsoftmax()